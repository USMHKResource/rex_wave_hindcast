{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins>DOE WPTO Wave Hindcast Data Access</ins>\n",
    "\n",
    "This notebook provides examples on accessing the 32-year Wave Hindcast dataset with brief examples on using the data within MHKiT and the SAMs tool. This data was generated via a collaboration between Pacific Northwest National Lab, Sandia National Lab, and the National Renewable Energy Lab and is hosted from Amazon Web Services using the HDF Group's Highly Scalable Data Service (HSDS) for public access.\n",
    "\n",
    "This dataset is continually growing and will contain data for all US territorial waters when complete.\n",
    "\n",
    "## <ins>Dataset Description</ins>\n",
    "\n",
    "The current available data spans the US West Coast spatially with >200 m resolution. The spatial domain extends out the EEZ, 200 nm and has been separated into federal and state controlled regions, accessable through the meta-data. The time step resolution for this spatial data is 3-hours and spans 32 years, 01/01/1979 - 12/31/2010. \n",
    "\n",
    "Virtual buoy dataset are also available at specific locations within the large spatial domain. These virtual buoys span the same 32-years of the spatial dataset however the time resolution is reduced to 1-hour. In addition to the shorter timestep, directional spectrum data is included in these dataset and is available.\n",
    "\n",
    "A list of the available variables in the spatial and virtual buoy datasets are listed below. \n",
    "\n",
    "## <ins>Dataset Variables</ins>\n",
    "\n",
    "### <ins>Spatial Dataset:</ins>\n",
    "\n",
    "Spatial datasets are located on AWS with the path: \"/nrel/US_wave/US_wave_${year}.h5\"\n",
    "\n",
    "#### West Coast Region:\n",
    "\n",
    "This spatial dataset is indexed by time and latitude/longitude coordinates ('time_index','coordinates'):\n",
    "\n",
    "- directionality_coefficient\n",
    "- energy_period\n",
    "- maximum_energy_direction\n",
    "- mean_absolute_period\n",
    "- mean_zero-crossing_period\n",
    "- omni-directional_wave_power\n",
    "- peak_period\n",
    "- significant_wave_height\n",
    "- spectral_width\n",
    "- water_depth\n",
    "\n",
    "### <ins>Virtual Buoy Dataset</ins>\n",
    "\n",
    "Virtual Buoy datasets are located on AWS with the path: \"/nrel/US_wave/virtual_buoy/US_virtual_buoy_${year}.h5\"\n",
    "\n",
    "#### West Coast Region:\n",
    "\n",
    "These virtual buoys are indexed by time and buoy latitude/longitude coordinates ('time_index','coordinates'). Directional variables have the extra indices of direction and frequency ('time_index','frequency','direction','coordinates')\n",
    "\n",
    "- directional_wave_spectrum\n",
    "- directionality_coefficient\n",
    "- energy_period\n",
    "- maximum_energy_direction\n",
    "- mean_absolute_period\n",
    "- mean_wave_direction\n",
    "- mean_zero-crossing_period\n",
    "- omni_directional_wave_power\n",
    "- peak_period\n",
    "- significant_wave_height\n",
    "- spectral_width\n",
    "- water_depth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <ins>Installing and Configuring the HSDS service</ins>\n",
    "\n",
    "For this example to work, the packages necessary can be installed via pip:\n",
    "\n",
    "pip install -r requirements.txt\n",
    "\n",
    "Next you'll need to configure h5pyd to access the data on HSDS:\n",
    "\n",
    "hsconfigure\n",
    "\n",
    "and enter at the prompt:\n",
    "\n",
    "hs_endpoint = https://developer.nrel.gov/api/hsds   \n",
    "hs_username = None   \n",
    "hs_password = None   \n",
    "hs_api_key = 3K3JQbjZmWctY0xmIfSYvYgtIcM3CN0cb1Y2w9bf    \n",
    "\n",
    "The example API key here is for demonstation and is rate-limited per IP. To get your own API key, visit https://developer.nrel.gov/signup/\n",
    "\n",
    "You can also add the above contents to a configuration file at ~/.hscfg\n",
    "\n",
    "Finally, you can use Jupyter Notebook or Lab to view the example notebooks depending on your preference\n",
    "\n",
    "## <ins>Access the HSDS server</ins>\n",
    "\n",
    "rex enables the efficient and scalable extraction, manipulation, and computation with NRELs flagship renewable resource datasets: the Wind Integration National Dataset (WIND Toolkit), and the National Solar Radiation Database (NSRDB). Development of functionality with the WPTO Wave Hindcast dataset is currently ongoing. MHKiT also leverages rex to access the WPTO Wave data to enable streamlined integrationg into that analysis ecosystem\n",
    "\n",
    "The WPTO Wave Hindcast Dataset is provided in annual .h5 files.\n",
    "\n",
    "Each year can be accessed from /nrel/US_wave/US_wave_${year}.h5\n",
    "\n",
    "To open the desired year of WPTO Wave Hindcast data server endpoint, username, password is found via a config file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick View of metadata, time_index, and coordinates \n",
    "from rex import WaveX\n",
    "\n",
    "waveFile = f'/nrel/US_wave/US_wave_1990.h5'\n",
    "\n",
    "with WaveX(waveFile, hsds=True) as waves:\n",
    "    meta = waves.meta          ## meta is an object that contains location information for each data point\n",
    "    print(\"meta =\", meta)\n",
    "    time_index = waves.time_index # time_index contains all of the years within the file\n",
    "    print(\"time_index =\",time_index)\n",
    "    coordinates = waves.coordinates # coordinates contains all the lat/lon pars within the dataset\n",
    "    print(\"coordinates =\",coordinates)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins>Basic Usage of the Spatial Dataset</ins>\n",
    "\n",
    "The following examples illustrate basic examples using NREL-rex to access and download parts of the WPTO Wave Hindcast dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <ins>Accessing the Datasets</ins> \n",
    "\n",
    "Below are some example of how you can access data using the NREL-rex Python package.\n",
    "Datasets are returned as Pandas objects. See the Pandas [documentation](https://pandas.pydata.org/pandas-docs/stable/) \n",
    "for more information about working with Pandas objects.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data from a single site:\n",
    "A single lat/lon pair can be given to extract data nearest to that location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the timeseries of significant_wave_height closest to a coordinate pair\n",
    "from rex import WaveX\n",
    "\n",
    "with WaveX(waveFile, hsds=True) as waves:\n",
    "    lat_lon = (44.624076,-124.280097)\n",
    "    parameters = 'significant_wave_height'\n",
    "    swh_single = waves.get_lat_lon_df(parameters, lat_lon)\n",
    "swh_single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data from multiple sites:\n",
    "A list of latitude/longitude pairs can be passed to extract data from multiple sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract the timeseries of significant_wave_height closest to multiple coordinate pairs\n",
    "from rex import WaveX\n",
    "\n",
    "with WaveX(waveFile, hsds=True) as waves:\n",
    "    lat_lon = [(44.624076,-124.280097),(43.489171,-125.152137)] # set lat_lon to a list of lat/lon pairs\n",
    "    parameters = 'significant_wave_height'\n",
    "    swh_multi = waves.get_lat_lon_df(parameters, lat_lon)\n",
    "swh_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all data within a specified region: \n",
    "\n",
    "Data can be extracted from a particular region based on jurisdiction regions found in the meta object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the timeseries of significant_wave_height in a region specified in the meta data\n",
    "from rex import WaveX\n",
    "\n",
    "with WaveX(waveFile, hsds=True) as waves:\n",
    "    region = 'Oregon'                     # Specify Oregon as the jurisdiction region \n",
    "    region_col = 'jurisdiction'           # specify jurisdiction as the meta object column you are searching against\n",
    "    variables = 'significant_wave_height'\n",
    "    swh_map = waves.get_region_df(variables, region, region_col=region_col)\n",
    "swh_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Data over Multiple Years:\n",
    "Data can be extracted over a number of years and concatonated directly using NREL-rex functionality.\n",
    "The new multi-year object has the same functionality as a single year dataset. The datasets are concatonated into a single timeseries for convenience "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatonate yearly significant wave height datasets into a single timeseries\n",
    "from rex import MultiYearWaveX # Yearly concatonation requires the MultiYearResource function\n",
    "\n",
    "multi_year_waves = f'/nrel/US_wave/US_wave_199*.h5' # file names can now accept Wildcards to specify which files to load, lists of filenames with specific years are also supported\n",
    "\n",
    "with MultiYearWaveX(multi_year_waves,hsds=True) as mYears:\n",
    "    lat_lon = (44.624076,-124.280097)\n",
    "    parameters = 'significant_wave_height'\n",
    "    swh_multi_year = mYears.get_lat_lon_df(parameters, lat_lon)\n",
    "    \n",
    "swh_multi_year\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This functionality extends to multiple locations and regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatonate yearly significant wave height datasets for multiple llocations into a single timeseries\n",
    "from rex import MultiYearWaveX \n",
    "\n",
    "multi_year_waves = f'/nrel/US_wave/US_wave_199*.h5' \n",
    "\n",
    "with MultiYearWaveX(multi_year_waves, hsds=True) as waves:\n",
    "    lat_lon = [(44.624076,-124.280097),(43.489171,-125.152137)] # set lat_lon to a list of lat/lon pairs\n",
    "    parameters = 'significant_wave_height'\n",
    "    swh_multi = waves.get_lat_lon_df(parameters, lat_lon)\n",
    "\n",
    "swh_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins>Basic Usage of the Virtual Buoys</ins>\n",
    "\n",
    "Virtual buoys were created during the larger UnSWAN model runs and contain 1-hour timestep data with directional spectrum data\n",
    "\n",
    "Virtual Buoy dataset fill names follow the convention: \"/nrel/US_wave/virtual_buoy/US_virtual_buoy_{year}.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the metadata and time_index for the virtual buoy datasets\n",
    "from rex import WaveX\n",
    "\n",
    "vBuoy = f'/nrel/US_wave/virtual_buoy/US_virtual_buoy_1995.h5'\n",
    "\n",
    "with WaveX(vBuoy, hsds=True) as waves:\n",
    "    meta = waves.meta          ## meta is an object that contains location information for each data point\n",
    "    print(\"meta =\", meta)\n",
    "    time_index = waves.time_index # time_index contains all of the years within the file\n",
    "    print(\"time_index =\",time_index)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Virtual Buoy datasets use the same functions which are used to select dataset from the spatial dataset, thus the previous examples apply to the virtual buoys.\n",
    "#### An example of gathering multi-year data for a single buoy is provided below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and concatonate multiple years of significant wave height data from teh virtual buoy dataset\n",
    "from rex import MultiYearWaveX\n",
    "\n",
    "multi_year_vBuoy = f'/nrel/US_wave/virtual_buoy/US_virtual_buoy_199*.h5'\n",
    "\n",
    "with MultiYearWaveX(multi_year_vBuoy,hsds=True) as mYears:\n",
    "    lat_lon = (44.624076,-124.280097)\n",
    "    parameters = 'significant_wave_height'\n",
    "    swh_buoy = mYears.get_lat_lon_df(parameters, lat_lon)\n",
    "    \n",
    "swh_buoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins>Integration with MHKiT</ins>\n",
    "\n",
    "Functionality to read the WPTO hindcast data is currently being integrated into [MHKiT](https://mhkit-software.github.io/MHKiT/), and will be avaiable to users soon. The following examples show how you will be able to use MHKiT to access the dataset once the functions are integrated into MHKiT. In addition to the Python functions demonstrated below, MHKiT-MATLAB functions will also be available with the same functionality.  \n",
    "\n",
    "Note: The following examples currently will NOT run on your local machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing local development version of MHKiT. This code will NOT work on local machines. \n",
    "import sys\n",
    "sys.path.insert(1, '/mnt/c/Users/abharath/Documents/Projects/MHKit/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data from a single site:\n",
    "A single lat/lon pair can be given to extract data nearest to that location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the timeseries of significant_wave_height closest to a coordinate pair\n",
    "from mhkit.wave import io\n",
    "\n",
    "single_year_waves = f'/nrel/US_wave/US_wave_1995.h5'\n",
    "lat_lon = (44.624076,-124.280097)\n",
    "parameters = 'significant_wave_height'\n",
    "\n",
    "wave_singleYear = io.read_US_wave_dataset(single_year_waves,parameters,lat_lon)\n",
    "\n",
    "wave_singleYear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data from multiple sites:\n",
    "A list of latitude/longitude pairs can be passed to extract data from multiple sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the timeseries of significant_wave_height closest to multiple coordinate pairs\n",
    "from mhkit.wave import io\n",
    "\n",
    "single_year_waves = f'/nrel/US_wave/US_wave_1995.h5'\n",
    "lat_lon = [(44.624076,-124.280097),(43.489171,-125.152137)]\n",
    "parameters = 'significant_wave_height'\n",
    "\n",
    "wave_multiLocal = io.read_US_wave_dataset(single_year_waves,parameters,lat_lon)\n",
    "\n",
    "wave_multiLocal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Data over Multiple Years:\n",
    "Data can be extracted over a number of years and concatonated directly with the MHKiT tools.\n",
    "The new multi-year object has the same functionality as a single year dataset. The datasets are concatonated into a single timeseries for convenience "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and concatonate a multi-year the timeseries of significant_wave_height closest to a coordinate pair\n",
    "from mhkit.wave import io\n",
    "\n",
    "multi_year_waves = f'/nrel/US_wave/US_wave_199*.h5'\n",
    "lat_lon = (44.624076,-124.280097)\n",
    "parameters = 'significant_wave_height'\n",
    "\n",
    "wave_multiYear = io.read_US_wave_dataset(multi_year_waves,parameters,lat_lon)\n",
    "\n",
    "wave_multiYear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar way to the pervious virtual buoy examples using the rex package, Virtual buoy data can be accessed through MHKiT-Python simply by selecting the correct file location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins>Working with Dataset Bulk Parameters</ins>\n",
    "\n",
    "In the following example we provide a simple means to view the data from the WPTO Wave Hindcast Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull datasets from AWS\n",
    "from mhkit.wave import io\n",
    "\n",
    "dataset = f'/nrel/US_wave/US_wave_1995.h5'\n",
    "lat_lon = (44.624076,-124.280097)\n",
    "swh_name, owp_name = 'significant_wave_height', 'omni-directional_wave_power'\n",
    "\n",
    "# First we use the MHKiT loading function to grab the datasets from AWS \n",
    "swh = io.read_US_wave_dataset(dataset,swh_name,lat_lon)\n",
    "owp = io.read_US_wave_dataset(dataset,owp_name,lat_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bfe422e4bc546bd83b8382db5a03ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%config InlineBackend.figure_format ='retina'\n",
    "%matplotlib widget\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "host = host_subplot(111)\n",
    "par = host.twinx()\n",
    "\n",
    "host.set_xlabel(\"Time\")\n",
    "host.set_ylabel(swh_name)\n",
    "par.set_ylabel(owp_name)\n",
    "\n",
    "p1, = host.plot(swh.index, swh.values, label=swh_name)\n",
    "p2, = par.plot(owp.index, owp.values, label=owp_name)\n",
    "\n",
    "leg = plt.legend()\n",
    "\n",
    "host.yaxis.get_label().set_color(p1.get_color())\n",
    "leg.texts[0].set_color(p1.get_color())\n",
    "\n",
    "par.yaxis.get_label().set_color(p2.get_color())\n",
    "leg.texts[1].set_color(p2.get_color())\n",
    "\n",
    "plt.title('WPTO Wave Hindacast Data for 1995')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins>Accessing and Working with the Virtual Buoy Direction Wave Spectum</ins>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Data must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-35d8aa5ae16b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# First we use the MHKiT loading function to grab the datasets from AWS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mspc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_US_wave_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspc_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlat_lon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mspc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/abharath/Documents/Projects/MHKit/mhkit/wave/io.py\u001b[0m in \u001b[0;36mread_US_wave_dataset\u001b[0;34m(wave_path, variable, lat_lon, tree, unscale, str_decode, hsds)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrex_accessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mwaveKwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwaves\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lat_lon_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlat_lon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/abharath/Documents/Projects/MHKit/mhkit/wave/io.py\u001b[0m in \u001b[0;36mread_US_wave_dataset\u001b[0;34m(wave_path, variable, lat_lon, tree, unscale, str_decode, hsds)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrex_accessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mwaveKwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwaves\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lat_lon_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlat_lon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.envs/rex_working/lib/python3.8/site-packages/rex/resource_extraction/resource_extraction.py\u001b[0m in \u001b[0;36mget_lat_lon_df\u001b[0;34m(self, ds_name, lat_lon)\u001b[0m\n\u001b[1;32m    459\u001b[0m         \"\"\"\n\u001b[1;32m    460\u001b[0m         \u001b[0mgid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlat_lon_gid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlat_lon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0msite_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gid_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msite_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.envs/rex_working/lib/python3.8/site-packages/rex/resource_extraction/resource_extraction.py\u001b[0m in \u001b[0;36mget_gid_df\u001b[0;34m(self, ds_name, gid)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \"\"\"\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             site_df = pd.DataFrame({ds_name: self[ds_name, :, gid]},\n\u001b[0m\u001b[1;32m    411\u001b[0m                                    index=self.time_index)\n\u001b[1;32m    412\u001b[0m             \u001b[0msite_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.envs/rex_working/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.envs/rex_working/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         ]\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.envs/rex_working/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_homogenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.envs/rex_working/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_homogenize\u001b[0;34m(data, index, dtype)\u001b[0m\n\u001b[1;32m    349\u001b[0m                     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_multiget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             val = sanitize_array(\n\u001b[0m\u001b[1;32m    352\u001b[0m                 \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             )\n",
      "\u001b[0;32m~/.envs/rex_working/lib/python3.8/site-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data must be 1-dimensional\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_tuplesafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Data must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "# pull datasets from AWS\n",
    "from mhkit.wave import io\n",
    "\n",
    "dataset = f'/nrel/US_wave/virtual_buoy/US_virtual_buoy_1995.h5'\n",
    "lat_lon = (44.624076,-124.280097)\n",
    "spc_name = 'directional_wave_spectrum'\n",
    "\n",
    "# First we use the MHKiT loading function to grab the datasets from AWS \n",
    "spc = io.read_US_wave_dataset(dataset,spc_name,lat_lon)\n",
    "spc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spc.to_pickle('../spc_example_dataset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capture Length and Power Matrices Calculations with MHKiT\n",
    "\n",
    "The WPTO hindcast data and MHKiT can be used to compute capture length and power matrices. The following example demonstrates this workflow using a synthetically generated power dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series\n",
    "from numpy.random import seed, normal\n",
    "\n",
    "owp = 'omni-directional_wave_power'\n",
    "J = io.read_US_wave_dataset(single_year_waves,owp,lat_lon)\n",
    "\n",
    "# Set the random seed, to reproduce results\n",
    "seed(1)                                               \n",
    "# Generate random power values\n",
    "P = Series(normal(200, 40, J.shape[0]),index = J.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mhkit.wave.performance import wave_energy_flux_matrix, capture_length, capture_length_matrix, power_matrix\n",
    "from numpy import arange\n",
    "\n",
    "Te = io.read_US_wave_dataset(single_year_waves,'energy_period',lat_lon)\n",
    "Hm0 = io.read_US_wave_dataset(single_year_waves,'significant_wave_height',lat_lon)\n",
    "\n",
    "# Calculate capture length\n",
    "L = capture_length(P, J) \n",
    "\n",
    "# Generate bins for Hm0 and Te, input format (start, stop, step_size)\n",
    "Hm0_bins = arange(0, Hm0.values.max() + .5, .5)    \n",
    "Te_bins = arange(0, Te.values.max() + 1, 1)\n",
    "\n",
    "# Create capture length matrix using a mean\n",
    "LM = capture_length_matrix(Hm0, Te, L, 'mean', Hm0_bins, Te_bins)\n",
    "# Create wave energy flux matrix using mean\n",
    "JM = wave_energy_flux_matrix(Hm0, Te, J, 'mean', Hm0_bins, Te_bins)\n",
    "\n",
    "# Create power matrix using mean\n",
    "PM = power_matrix(LM, JM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mhkit.wave.graphics import plot_matrix\n",
    "# Plot the Plot mean matrix\n",
    "ax = plot_matrix(PM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
